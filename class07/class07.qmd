---
title: "Class07: Machine Learning 1"
author: "Caliope Marin (PID: A13912583)"
format: pdf
---
Before we get into clustering methods lets make some sample data to cluster where we know that the answer should be. 

To help with this I will use the `rnorm()`function.
```{r}
hist(rnorm(150000, mean=-3))

```

```{r}
n=10000
hist( c(rnorm(n, mean=3), rnorm(n, mean=-3)))
```
```{r}
n=30
c( rnorm(n, mean=3), rnorm(n, mean=-3) )
```
```{r}
n=30
x <- c(rnorm(n, mean=3), rnorm(n, mean=-3) )
y <- rev(x)

z <- cbind(x, y) #rbind can combine data fram argument by columns and cbind combines by columns 
z
plot(z)
```


##K means clustering

The function in base R for k-means clustering is called `k-means()`. 

```{r}
km <-kmeans(z, centers=2) # assigns data points to the center data points 
#c;uster size is 30 because n is equal to 30, 
#cluster vector for each point its measuring which cluster is # closer to the center
```

```{r}
km$centers
```

Q. Print out the cluster membership vector (i.e our main answer)
```{r}
km$cluster
```
```{r}
plot(z, col=c("red", "blue")) #if you do this it will repeat each point 
```
```{r}

#you can color it by number 

plot(z, col=2)
```
Plot with clustering result

```{r}
# in order to separate the colors of each cluster
plot(z, col=km$cluster)
points(km$centers, col="blue", pch=15, cex=2) #pch=15 makes a #filled blue square
#cex stands for character expansion so greater than 1 makes 
#it greater thsn the characters
```

Q. Can you cluster our data in `z` into four clusters please? 

```{r}
km4 <- kmeans(z, centers = 4)
plot(z, col=km4$cluster)
points(km4$centers, col="blue", pch=15, cex=2)
```

##Hierarchical Clustering 

The main function for hierarchical clustering in base R is called `hclust()`

Unlike `kmeans()` I cannot just pass in mt data as input, I first need a distance matrix from my data. 

```{r}
d <-dist(z)
hc <-hclust(d)
hc
```

There is a specific hclut plot () method...

```{r}
plot(hc)
#all the numbers from 1-30 are on onside and the other is #above 30 
abline(h=10, col="red")
```

To get my main clustering result(i.e. the membership vector) I can "cut" my tree at a given height. To do this, I will use the `cutreee()`

```{r}
grps <- cutree(hc, h=10)
plot(z, col=grps)
```

#Principal Component Analysis

Principal component analysis (PCA) is a well established "multivariate statistical technique" used to reduce the dimensionality of a complex data set to a more manageable number (typically 2D or 3D). This method is particularly useful for highlighting strong paterns and relationships in large datasets (i.e. revealing major similarities and diferences) that are otherwise hard to visualize.

## PCA of UK food data

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names=1)
```

Q. 1 How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?
```{r}
#I can use `nrow()` and `ncol()` to show number of #rows and columns
nrow(x)
ncol(x)
```
##preview the first 6 rows

```{r}

head(x)
```
```{r}
# Note how the minus indexing works
rownames(x) <- x[,1]
x <- x[,-1]
head(x)
```

```{r}
dim(x)
```
```{r}
x <- read.csv(url, row.names=1)
head(x)
```
```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```


```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x))) #making `beside=F` will
#make the plot stack all the categories on top of eachother,not pleasant to #look at
```


```{r}
pairs(x, col=rainbow(10), pch=16)
```

#PCA to the rescue

The main function to do PCA in base R is called `prcomp()`. 

Note that I need to take the transpose of this particular data as that is what the `prcomp()` help page was asking for. 

```{r}
pca <- prcomp( t(x) ) #t stands for #transpose of x
summary(pca)
```
Whats inside of the object `pca`
```{r}
attributes(pca)

#use x to plot main result figure
pca$x
```


to make our main result figure, called a "PC plot" (or "score plot", "ordination plot" or "PC1 vs PC2 plot"). 
```{r}
# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2],  col= c( "black", "red", "blue", "darkgreen"), pch=16, xlab="PC1 (67.4%)", ylab="PC2 (29%)") 
abline(h=0, col="gray", lty=2)
abline(v=0, col="gray", lty=2)
text(pca$x[,1],pca$x[,2], colnames(x))
```
```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
## or the second row here...
z <- summary(pca)
z$importance

barplot(v, xlab="Principal Component", ylab="Percent Variation")
```


```{r}

par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

#Variable Loadings Plot

Can give us insight on how the original variables (in this case the foods contribute to our new PC axis)

```{r}
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
barplot( pca$rotation[,2], las=2)
```

