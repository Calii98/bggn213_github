---
title: "class 8: PCA Mini Project"
author: "Caliope Marin (PID: A13912583)"
format: pdf
---

Side_Note

```{r}
head(mtcars)
```
Let's look at the mean value of every column: 

```{r}
apply(mtcars, 2, mean)
```

Lets look at "spread" via `sd()`

```{r}
apply(mtcars, 2, sd)
```
```{r}
pca <- prcomp(mtcars)
biplot(pca)
```
We could do this with PCA 

Lets try scaling the data : 

```{r}
mtscale <- scale(mtcars)
head(mtscale)
```

What is the mean of each "dimension"/column in mtscale?

```{r}

round(apply(mtscale, 2, mean), 3)

```
```{r}
round(apply(mtscale, 2, sd), 3)
```

Let's plot `mpg` vs `disp` for both mtcars and after the scaled fata in `mtscale`

```{r}
library(ggplot2)

ggplot(mtcars) + 
  aes(mpg, disp) + 
  geom_point()
```

```{r}
ggplot(mtscale) + 
  aes(mpg, disp) + 
  geom_point()
```

# They look exactly the same. They are just standard by 0 
# Doesn't change the data it just scales it.

```{r}
pca2 <- prcomp(mtscale)
biplot(pca2)
```

#better representation of the data-- it hasnt been scaled to the vector with the biggest frame.

##Breast Cancer FNA data

```{r}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"
# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)

head(wisc.df)
```

```{r}
#remove the first column
wisc.data <- wisc.df[,-1]
```

```{r}
# Create diagnosis vector for later 
diagnosis <- as.factor(wisc.df$diagnosis)
```



>Q1. How many observations= rows/patients/sibjects are in the data set? 

```{r}
nrow(wisc.data)
```

#there are 569 observations#

>Q2. How many M (cancer) B (healthy) patients

```{r}
table(wisc.df$diagnosis)
```
#there are 357 healthy patients and 212 cancer patients

Be sure to remove this diagnosis column from our data to analyze
```{r}
#remove the first column
wisc.data <- wisc.df[,-1]
# Create diagnosis vector for later 

#factor is categorical 

```
>Q3. How many variables/features in the data are suffixed with _mean?

```{r}
 columns <-colnames(wisc.data)

length(grep( "_mean",x=columns, value=T))


```
#there are 10 columns with "_mean"

##PCA 

We want to scale out data before PCA by setting the `scale=TRUE`

```{r}

wisc.pr <- prcomp(wisc.data, scale=TRUE)
# Check column means and standard deviations
colMeans(wisc.data)

apply(wisc.data,2,sd)
```
How much variance captured in each PC

```{r}

summary(wisc.pr)

# the total variance is not as high/moderate for PC's
```
>Q4. Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

#PC1 proportion of variance = 44.27%

>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
#The first 3 PC1-3

>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
# the first 7 PC1-7

```{r}
x <- summary(wisc.pr)
x$importance

```
```{r}

plot(x$importance[2,], typ="b")

```

```{r}
biplot(wisc.pr)
```

>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?
This plot has a lot of noise ande it is too hard to read and understand. 

```{r}
attributes(wisc.pr)
```

```{r}
head(wisc.pr$x)
```
#My main PC result figure
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis, xlab="PC1", ylab="PC2")
# each point represents a patient and the color represents whether its red malignant or black healthy
```

```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[,1], wisc.pr$x[,3], col=diagnosis, xlab="PC1", ylab="PC3")
```

>Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?
There is more overlap between PC1 and PC2

Lets make a ggplot 
```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) +
  geom_point()
```
# Its telling me that these benign samples have a greater difference than cancer samples
#each point represents a sample and all of the transcripts

##Variance explained:
```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
# Variance explained by each principal component: pve
pve <-  pr.var/sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

```{r}
## ggplot based graph
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```
>Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean? 
#This tells us how much this original feature contributes to the first PC.

```{r}
wisc.pr$rotation[,1]
```


```{r}
wisc.pr$rotation[,1]["concave.points_mean"]
```

```{r}
loadings <- wisc.pr$rotation
#this can allow us to see percent variations in PCAs
ggplot(loadings)+ 
  aes(abs(PC1), reorder(rownames(loadings), -PC1))+
  geom_col()
```

##Hierarchical clustering

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
```


```{r}
data.dist <- dist(data.scaled, method="euclidean")
```

```{r}
wisc.hclust <- hclust(data.dist, method="complete")
```

##results of hierarchical clustering

```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)

#q11
```
##Selecting number of clusters

```{r}
wisc.hclust.clusters <-cutree(wisc.hclust, h=19)
table(wisc.hclust.clusters, diagnosis)
```

>Q11.  OPTIONAL: Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10? How do you judge the quality of your result in each case?

#This is pretty difficult to decipher between cutting into different clusters for each diagnosis because the more clusters the harder to determine the correct points vs clustering into might lead you to greater chance of false positives. 

```{r}
wisc.hclust.clusters <-cutree(wisc.hclust, k=2)
table(wisc.hclust.clusters, diagnosis)

```

 
```{r}

```

## Clustering

Try to cluster tje `wisc.data`

```{r}
km <- kmeans(wisc.data, centers =2)
table(km$cluster)
```

```{r}
d <- dist(wisc.data)
hc <- hclust(d)
plot(hc)
```

```{r}
grps<- cutree(hc, k=3)
table(grps)
```

##Cluster in PC space 

In other words use my PCA results as a basis of clustering

```{r}
head(wisc.pr$x)

##Q12
d <- dist(wisc.pr$x[,1:3])
hc <- hclust(d, method="ward.D2")
#claculates distance 
plot(hc)
```
 >Q12.  Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.
#The method that gives me the best results is using "ward.D2" because it considers each point distance between the centroids of the clusters being merged giving the best clustering method for PCA data.
```{r}
grps <-cutree(hc, k=2)
table(grps)
```



Compare to my expert M and B `diagnosis`
```{r}
table(diagnosis)
table(diagnosis, grps)
```
#we're getting 33 false pos for malignant and 24 false pos for benign
# how can we be 100% sensitive ??



```{r}
plot(wisc.pr$x[,1:2], col=grps)
```

```{r}
plot(wisc.pr$x[,1:2],col=diagnosis)
```


```{r}
g <-as.factor(grps)
levels(g)

```

```{r}
g <-relevel(g,2)
levels(g)
```

```{r}
#plot using our re-ordered factor
plot(wisc.pr$x[,1:2], col=g)

```

##Let's make it 3D!!

```{r}
#library(rgl)
#plot3d(wisc.pr$x[,1:3], xlab="PC 1", ylab="PC 2", zlab="PC 3", #cex=1.5, size=1, type="s", col=grps)
```






